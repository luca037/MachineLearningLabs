{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Decision Tree on Complete Diamonds Price Dataset\n",
    "\n",
    "The **Diamonds dataset** from Kaggle is a dataset containing information about the physical and pricing attributes of nearly 54,000 diamonds.\n",
    "\n",
    "Compared to Homework 1, in this case we will consider 8,000 samples of the **complete** dataset (i.e. without excluding categorical variables).\n",
    "\n",
    "### Key Features:\n",
    "- **Carat**: The weight of the diamond.\n",
    "- **Cut**: Quality of the cut.\n",
    "- **Color**: Diamond colour.\n",
    "- **Clarity**: The measurement of how clear the diamond is.\n",
    "- **Depth**: The total depth percentage (z / mean(x, y)).\n",
    "- **Table**: Width of the diamond's top as a percentage of its widest point.\n",
    "- **Price**: Price in US dollars.\n",
    "- **X, Y, Z**: Dimensions of the diamond in mm (length, width, depth).\n",
    "\n",
    "This dataset is useful for exploring relationships between physical attributes and pricing, and for building predictive models to estimate diamond prices based on their features.\n",
    "\n",
    "For more information see: https://www.kaggle.com/datasets/shivam2503/diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In the notebook you will perform a complete pipeline of decision tree - regression task. \n",
    "First, you will:\n",
    "- perform the ordinal encoding of categorical variables;\n",
    "- split the data into training and test set;\n",
    "- standardize the data.\n",
    "\n",
    "You will then be asked to learn various decision tree models. \n",
    "\n",
    "1. Start by training a decision tree without any limitations (i.e., leaving the **default hyperparameters**).\n",
    "2. Next, try to set a **different value for max_depth** hyperparameter to see what happens.\n",
    "3. Then, identify the optimal max_depth through **cross-validation**.\n",
    "5. Learn the decision tree with optimal max_depth found above.\n",
    "6. Inspect the importance of each feature and print the name of the best ones.\n",
    "7. Compare the best decision tree obtained above with a standard Linear Regressor.\n",
    " \n",
    "### IMPORTANT.\n",
    "- Note that in each of the above steps you will have to choose the appropriate split of the data (see the second bullet point above);\n",
    "- The code should run without requiring modifications even if some best choice of parameters changes; for example, you should not pass the best value of hyperparameters \"manually\" (i.e., passing the values as input parameters to the models). The only exception is in the TO DO titled 'ANSWER THE FOLLOWING'\n",
    "- Do not change the printing instructions (other than adding the correct variable name for your code), and do not add printing instructions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- Insert your ID number (\"numero di matricola\") below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- put here your ID Number (\"numero di matricola\")\n",
    "numero_di_matricola = # -- TO DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- import some packages\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- load the data (csv format)\n",
    "df = pd.read_csv('diamonds.csv', sep = ',')\n",
    "\n",
    "# -- remove the data samples with missing values (NaN)\n",
    "df = df.dropna()\n",
    "\n",
    "# -- drop the column containing the id of the data\n",
    "df = df.drop(columns=['Unnamed: 0'], axis=1)\n",
    "\n",
    "# -- print the column names together with their data type\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we convert our (pandas) dataframe into set X (containing our features) and the set Y (containing our target, i.e., the price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- compute X and Y sets\n",
    "X = df.drop(columns=['price'], axis=1)\n",
    "Y = df['price']\n",
    "\n",
    "print(\"Total number of samples:\", X.shape[0])\n",
    "\n",
    "# -- print the features names\n",
    "features_names = list(X.columns)\n",
    "print(\"Features names:\", features_names)\n",
    "\n",
    "X = X.values\n",
    "Y = Y.values\n",
    "\n",
    "# -- print shapes\n",
    "print('X shape: ', X.shape)\n",
    "print('Y shape: ', Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- ORDINAL ENCODING OF CATEGORICAL VARIABLES\n",
    "\n",
    "Use the $\\texttt{preprocessing.OrdinalEncoder}$ from scikit learn to perform ordinal encoding of the three categorical variables: **cut**, **color**, and **clarity**.\n",
    "\n",
    "***NOTE***: Use the input parameter $\\texttt{categories}$ to specify, from worst to best, the levels for each categorical variable. In detail:\n",
    "- Cut: ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
    "- Color: ['J', 'I', 'H', 'G', 'F', 'E', 'D']\n",
    "- Clarity: ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
    "\n",
    "For more information see: https://www.kaggle.com/datasets/shivam2503/diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first print the data type of each column, returning both column name and its corresponding index in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print the data type of each column\n",
    "for index_col, name_col in zip(range(X.shape[1]), features_names):\n",
    "    print(f\"Column {name_col} (index: {index_col}) -- data type: {type(X[0, index_col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's encode the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the encoding was done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print the data type of each column\n",
    "for index_col, name_col in zip(range(X.shape[1]), features_names):\n",
    "    print(f\"Column {name_col} (index: {index_col}) -- data type: {type(X[0, index_col])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- SPLIT DATA INTO TRAINING AND TEST SET, WITH THE FOLLOWING PERCENTAGES: 80% AND 20%\n",
    "\n",
    "First, compute the number of samples to be included in the training set (i.e., 80% of the data) and the number of samples to be included in the test set (i.e., 20% of the data) and print such values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- split data into train (4/5 of samples) and test data (1/5 of samples)\n",
    "\n",
    "# -- TO DO\n",
    "\n",
    "print(\"Amount of data for training and deciding parameters:\", # -- TO DO)\n",
    "print(\"Amount of data for test:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the $\\texttt{train\\_test\\_split}$ function from sklearn.model_selection to split the data; in every call fix $\\texttt{random\\_state}$ to your numero_di_matricola. \n",
    "At the end, you should store the data in the following variables:\n",
    "- X_train, Y_train: training data;\n",
    "- X_test, Y_test: test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- DATA STANDARDIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stardardise the data using $\\texttt{preprocessing.Standardscaler}$ from scikit learn.\n",
    "\n",
    "If V is the name of the variable storing part of the data, the corresponding standardized version should be stored in V_scaled. For example, the scaled version of X_train should be stored in X_train_scaled.\n",
    " \n",
    "For simplicity, with the function $\\texttt{copy}$, create a copy of the variable V by calling it V_scaled and then apply the scaler to this copy.\n",
    "\n",
    "***NOTE***: standardise only the 6 continuous variables (**carat**, **depth**, **table**, **x**, **y**, **z**) and not the 3 categorical variables just encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- data standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are supervised machine learning models used for both **classification** and **regression** tasks. They are structured like a tree, where each node represents a condition on the data, each branch corresponds to a possible answer, and the leaves represent the final outcome (class or value). In this homework, you will use decision trees in a regression setting to predict the price of diamonds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- DEFAULT SETTINGS\n",
    "\n",
    "Learn a decision tree leaving the default values for the hyperparameters. Set only $\\texttt{random\\_state}$ to your numero_di_matricola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the value of 1 - coefficient of determination $(R^2)$, to evaluate how well the model fits both the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1 - coefficient of determination on training data:\", # -- TO DO)\n",
    "print(\"1 - coefficient of determination on test data:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what are some of the characteristics of the tree, like its depth and the number of nodes.\n",
    "\n",
    "In detail, print:\n",
    "- ***max_depth***: limits the maximum depth of the tree, controlling how many splits it can make.\n",
    "- ***node_count***: represents the total number of nodes in the tree, including both internal nodes and leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Depth of the tree:\", # -- TO DO)\n",
    "print(\"Number of nodes:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- ANSWER THE FOLLOWING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question (max 500 characters):\n",
    "\n",
    "Based on the 1 - coefficient of determination $(R^2)$ values on training and test set and based on the max_depth/node_count, what conclusions could you draw regarding the trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANSWER\")\n",
    "\n",
    "# -- the following is a string with you answer\n",
    "# -- TO DO\n",
    "motivation = \"My answer ...\"\n",
    "\n",
    "print(motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- SET A DIFFERENT VALUE FOR max_depth\n",
    "\n",
    "Now, try with a different value for $\\texttt{max\\_depth}$ hyperparameter. Set it equals to 2 and $\\texttt{random\\_state}$ to your numero_di_matricola, than fit the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the error obtained by this model on both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- print the value of 1 - coefficient of determination R^2, for the training and test data\n",
    "print(\"1 - coefficient of determination on training data:\", # -- TO DO)\n",
    "print(\"1 - coefficient of determination on test data:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "plt.figure()\n",
    "tree.plot_tree(decision_tree = # -- TO DO, \n",
    "               feature_names=features_names, \n",
    "               class_names=['price'], \n",
    "               filled=True)\n",
    "plt.savefig('tree.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- ANSWER THE FOLLOWING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question (max 500 characters):\n",
    "\n",
    "Based on the 1 - coefficient of determination $(R^2)$ values on training and test set and based on the max_depth/node_count, what conclusions could you draw regarding this new trained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANSWER\")\n",
    "\n",
    "# -- the following is a string with you answer\n",
    "# -- TO DO\n",
    "motivation = \"My answer ...\"\n",
    "print(motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- DECISION TREE WITH CROSS-VALIDATION FOR max_depth TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform $k$-fold cross validation (with $k$ = 5) with respect to the parameter $\\texttt{max\\_depth}$, with $\\texttt{max\\_depth}$ ranging from 1 to 30 included.\n",
    "\n",
    "**Note**: consider only **integer** values for $\\texttt{max\\_depth}$!\n",
    "\n",
    "At the end, note that you need to store in $\\texttt{max\\_depth\\_opt}$ the best value for $\\texttt{max\\_depth}$ you found with the cross-validation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# -- define the grid for the max_depth hyperparameter\n",
    "\n",
    "max_depth_grid = # -- TO DO\n",
    "\n",
    "# -- initialize the vector for the errors (1 - R^2)\n",
    "\n",
    "err_train_kfold = # -- TO DO\n",
    "err_val_kfold = # -- TO DO\n",
    "\n",
    "# -- perform kfold cross validation for model selection (k = 5)\n",
    "\n",
    "# -- TO DO\n",
    "\n",
    "# -- choose the regularization parameter that minimizes the loss\n",
    "\n",
    "max_depth_opt = # -- TO DO\n",
    "print('Best value of the max_depth parameter:', max_depth_opt)\n",
    "print('Min. validation error (1 - R¬≤) ', # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot validation and test error (1 - $R^2$) for different values of $\\texttt{max\\_depth}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- plot the training error\n",
    "plt.plot(max_depth_grid, \n",
    "         err_train_kfold, \n",
    "         color='r', \n",
    "         marker='x')\n",
    "\n",
    "# -- plot the validation error\n",
    "plt.plot(max_depth_grid, \n",
    "         err_val_kfold, \n",
    "         color='b', \n",
    "         marker='x')\n",
    "\n",
    "# -- highlight min loss\n",
    "plt.scatter(max_depth_opt, \n",
    "            np.min(err_val_kfold), \n",
    "            color='b', \n",
    "            marker='o', \n",
    "            linewidths=5)\n",
    "\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Error')\n",
    "plt.title('DecisionTree: choice of max_depth parameter')\n",
    "plt.savefig('train_val_loss.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn the final model using the optimal _max_depth_ obtained above and print the error (1 - R¬≤) of the model on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- TO DO\n",
    "\n",
    "print(\"1 - coefficient of determination on training data:\", # -- TO DO)\n",
    "print(\"1 - coefficient of determination on test data:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO -- FEATURE IMPORTANCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the importance of each feature for the best decision tree obtained using the property $\\texttt{feature\\_importances\\_}$ of $\\texttt{DecisionTreeRegressor}$ class. The importance of a feature is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(# -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the names of the three most importante features. To do this:\n",
    "1. Get the indexes of the top-3 features according to their importance;\n",
    "2. Print the name of each of the top-3 feature using the format \"feature_name feature_index\" (e.g., \"depth 4\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- get the indexes of the top-3 features\n",
    "# -- TO DO\n",
    "\n",
    "# -- print the name of each of the top-3 features\n",
    "# -- TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO -- ANSWER THE FOLLOWING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following question (max 500 characters):\n",
    "\n",
    "What are some advantages of using a Decision Tree with respect to a Deep Neural Network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nANSWER\")\n",
    "\n",
    "# -- the following is a string with you answer\n",
    "motivation = \"My Answer ...\"\n",
    "print(motivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO -- COMPARISON WITH LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Linear Regression model and compare it with the best decision tree obtained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# -- TO DO\n",
    "\n",
    "print(\"Linear Regression training error:\", # -- TO DO)\n",
    "print(\"Linear Regression test error:\", # -- TO DO)\n",
    "print(\"Decision Tree training error:\", # -- TO DO)\n",
    "print(\"Decision Tree test error:\", # -- TO DO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ABOUT TREES ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def christmas_tree():\n",
    "    \n",
    "    tree_height = 16\n",
    "    \n",
    "    print(\"\\n\\nüéÑ Merry Christmas! üéÑ\\n\\n\".center(40))\n",
    "          \n",
    "    for i in range(tree_height):\n",
    "    \n",
    "        spaces = \" \" * (tree_height - i - 1)\n",
    "        \n",
    "        if i == 0:\n",
    "            layer_content = \"‚≠ê\"\n",
    "        else:\n",
    "            contents = [ \"üçÉ\", \"üü°\", \"üî¥\", \"üîµ\"]\n",
    "            content = random.choices(contents, weights = [0.7, 0.1, 0.1, 0.1], k = 2 * i + 1)\n",
    "            delimiter = \"\"\n",
    "            layer_content = delimiter.join(content)\n",
    "            \n",
    "        print((spaces + layer_content).center(40))\n",
    "            \n",
    "    trunk = \" \" * (tree_height - 1) + \"üü´\"\n",
    "    print(trunk.center(40))\n",
    "\n",
    "christmas_tree()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
